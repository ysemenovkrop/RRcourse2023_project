---
title: "RR Project Report"
author: "Ahmed, Semenov, Singh"
date: "06/18/2023"
format:
  html: 
    toc: true
    toc-depth: 2
    toc-expand: 3
    toc-title: Contents
    toc-location: left
theme: solar
editor: visual
execute:
  echo: false
  warning: false
  messages: false
---

## I. Original research

*The original research aimed to investigate the relationship between US housing prices and US consumer confidence level, quantified by the University of Michigan Consumer Sentiment Index. In an attempt to combine classical economic theory with behavioral theory, the research hypothesized that consumer confidence (quantified) had a positive influence on the changes in housing market. Towards conducting such an analyses, a Vector Autoregressive model was implemented, with two target variables - HPI, a broad measure of the movement of single-family house prices possibly deviating from its fundamental value (common definition of an "asset bubble") and CONF, representing the confidence level of US consumers.*

## II. Reproduction of results (by translation of code from E-Views into R language)

```{r}
# suppressing the warnings
options(warn = -1)

# setting the working directory
setwd("E:/Priv/2022-2023^/RR/Project/Data")

# installing libraries
install.packages("fma")

# loading libraries
library(readxl)
library(dplyr)
library(xts)
library(urca)
library(vars)
library(stats)
library(fma)

# loading external function
source("../ARMA_function.R")
```

## 1. Data pre-processing

*Thanks to the original research presenting sufficient information regarding model variables and their sources, we were able to download/acquire the necessary data. However the original research does not mention anything on data pre-processing methods used. Hence, relying on subjective intuition we exclude missing (NA) values on the sourcing level (i.e raw data) and proceed towards adjusting the frequency of the data (quarterly frequency) and concatenating it.*

| Variable   |          Brief Description                         |            Source                            |
|:-----------|:--------------------------------------------------:|:--------------------------------------------:|
| **HPI**    | Housing Pricing Index (1980 =100)                  | Federal Reserve Economic Data                |
| **GDP**    | Gross Domestic Product (current US$, billions      | Federal Reserve Economic Data                | 
| **CONF**   | Michigan Confidence Index (1964 = 100)             | Surveys of consumers, University of Michigan | 
| **DSPI**   | Real Disposable Personal Income,(chained 2009 US$) | Federal Reserve Economic Data                |
| **CPI**    | Consumer Price Index (1984=100)                    | Federal Reserve Economic Data                |
| **UNRATE** | Unemployment (% out of total labor force)          | Federal Reserve Economic Data                |
| **POP**    | Total population (All ages, thousands)             | Federal Reserve Economic Data                |
| **IR**     | Interest rate of 10 Yr US Treasury Bond            | Federal Reserve Economic Data                |

```{r}
# creating a vector with the names of the raw files

data <- c("CPI", "DSPI", "HPI", "IR", "POP", "UNRATE", "CONF", "GDP", "MORTG")

# loading the data

for (i in data) {
  filename <- paste0(i, ".xls")
  file_path <- paste0("../Raw data",
                      filename)
  assign(i, read_excel(file_path))
}

# checking the structure of the data

data_2 <- list(CPI = CPI, DSPI = DSPI, HPI = HPI, IR = IR, POP = POP,
               UNRATE = UNRATE, CONF = CONF)
for (i in names(data_2)) {
  x = str(data_2[[i]])
  print(paste(i, as.character(x)))
}

## standardization of the data
# list to loop through

data_3 <- list(CPI = CPI, DSPI = DSPI, HPI = HPI, IR = IR, POP = POP,
               UNRATE = UNRATE)

# steps for the loops:
# convert to data frame
# standardize the column names as "Date" + "name of the variable"
# convert to the xts object
# aggregate to quarter basis

for (i in names(data_3)) {
  df <- as.data.frame(data_3[[i]])
  names(df) <- c("Date", i)
  df_xts <- xts(df[,2], order.by=df$Date)
  df_q <- to.quarterly(df_xts)[, 4]
  assign(paste0(i, "_q"), df_q)
}

# as CONF, GDP and MORTG are downloaded with quarterly frequency, adjustment is made outside the the main loop

# confidence
CONF <- CONF[2]
CONF <- data.frame(CONF).
# producing a vector that has all numeric values for CONF
CONF <- as.numeric(unlist(CONF))

# GDP
GDP <- GDP[2]
GDP <- data.frame(GDP)
# producing a vector that has all numeric values for GDP_2
GDP <- as.numeric(unlist(GDP))

# Mortgage
MORTG <- MORTG[2]
MORTG <- data.frame(MORTG)
# producing a vector that has all numeric values for CONF
MORTG <- as.numeric(unlist(MORTG))

# creating the final dataset with all variables

dataset <- merge(CPI_q, DSPI_q, GDP, HPI_q, IR_q, POP_q, UNRATE_q, CONF, MORTG)
names(dataset) <- c("CPI", "DSPI", "GDP", "HPI", "IR", "POP", "UNRATE", "CONF",
                    "MORTG")

# removing unnecessary objects

rm(CPI, DSPI, GDP, HPI, IR, POP, UNRATE,CONF, MORTG, data_2, data_3, df, df_q, df_xts, CPI_q,
   DSPI_q, HPI_q, IR_q, POP_q, UNRATE_q, data, file_path, file_name,
   i,x, filename)

# saving the preparing data file as an R object
#save(dataset, file = "dataset")
```

```{r}
#loading data set prepared by Semenov
#load("dataset") 
```

## 2. Modelling

***(a).** Stationarity of variables,*
          
*The original research uses unit root tests to discover information concerning the stationarity of the variables in the model. After first differences are taken, the Inverse Roots of AR Characteristic Polynomial, showed that there was no non-stationarity in the model as all values lied within the unit root circle. Our reproduction shows the same results.*

```{r}
## taking first differences of the variables
# loop for taking the first difference of the

for (i in 1:ncol(dataset)) {
  dataset[,i] <- diff.xts(dataset[,i], lag = 1)
}

# removing NAs
dataset <- dataset[-1, ]
colSums(is.na(dataset))
```

```{r}
## Inverse Roots of AR Characteristic Polynomial
# using the exported carrots function plotting the inverse roots of AR
# characteristic polynomial
```
```{r}
plot(arroots(ar.ols(dataset$GDP)))
```
```{r}
plot(arroots(ar.ols(dataset$IR)))
```
```{r}
plot(arroots(ar.ols(dataset$HPI)))
```
```{r}
plot(arroots(ar.ols(dataset$DSPI)))
```
```{r}
plot(arroots(ar.ols(dataset$POP)))
```
```{r}
plot(arroots(ar.ols(dataset$UNRATE)))
```
```{r}
plot(arroots(ar.ols(dataset$MORTG)))
```
```{r}
plot(arroots(ar.ols(dataset$CONF)))
```

***(b).** VAR model,*.

*For choosing the proper lag length for VAR, the original research used multivariate information criteria. Different models were estimated and compared for a maximum number of 3 lags. In the original research, the majority of AIC, HQ, SC and FPE selection criteria were best for lag nr. 3, however our results indicate a 50/50 split favoring lag nr 1 and lag nr 3. This could be due to differences in the data sample as a consequence of different data pre-processing methods.*

```{r}
VARselect(dataset,
          lag.max = 3)
```
```{r}
```

***(c).** Correlation between variables in the model,*

*The original research showed correlation above 0.5 only between HPI and MORG. Our reproduction...*

```{r}

```

***(d).** Granger causality test,*

***(e).** Impulse responses,*

## III. Comparison of results

## IV. Improvements

***(a).** More data*

***(b).** Cointegration between variables in the model*

**Cointegration is tested on non-stationary data**

***(c).** Forecast error variance decomposition*

## V. Summary and Conclusions

*Multivariate time series analysis aims at taking into account long-run relationship between time series.Trends should be removed due to possible spurious regression problem. It causes loosing information about the mentioned long-run relationship. One need to include it in the analysis in some other way.*

*One idea is to think of causality (or rather Granger causality) between the series and simply treat all variables as exogenous - VAR*

*Disadvantages are that VAR models are a-theoretical (similarly to ARMA models). They use little theoretical information about the relationships between the variables. Estimating VARs often seems to rely on the rule: let the data decide. As a result, there exists an increased possibility to obtain an essentially spurious relationship within the VAR approach (mining the data).*